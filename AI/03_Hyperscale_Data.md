# 교수님 코멘트

데이터를 그냥 모으기만 하면 안되고! 

- Train data 

- Validation data -> 간단히 검증된 데이터 -> 중간중간 평가를 해주면서 성능을 높여주는 용도? -> pre test 

- Test data -> 테스트 때와 비슷한 유형인데, 다른 데이터 

이렇게 나눠서 관리를 해야 한다.. 

공공데이터는 이미 나눠져 있죠? 



사진 몇 만 장이 있다! 

반드시! 나눠서 진행해야 합니다 

자연어 처리도 비슷 

 

데이터 전체를 가지고 학습해버리면.. 

over fitting 문제 발생 ㄱㅏ능성 

 

테스트에 넣었던 데ㅇㅣ터를 가지고 validation에 넣으면 

당연히 100퍼센트 나와요 

AI를 처음 개발하는 사람들이 흔히 하는 실수.. 

---

Iteration == epoch 

기본 500번 반복한다고 합니다 

데이터양이 커지면 커질수록, epoch 1,000, 2,000 . . . optimization 

이걸 정해주는 게 반드시 있을 겁니다! 

해당된 데ㅇㅣ터에 대해서 prediction 

 

분류했는데 성능 낮다 싶으면 이 epoch 값을 늘려주면 되겠지요 

# 초거대 데이터의 개요

## 빅데이터 Big Data

- 디지털 환경에서 생성되는 데이터

- 과거 아날로그 환경에서 생산되던 데이터에 비해

- 규모가 아주 방대

- 생성 주기도 짧음

- 수치 데이터 뿐만 아니라, 문자나 영상 데이터도 포함하는

- 대 규 모 데 이 터

- 빅데이터 = 사람이 생성한 데이터 + 기계가 생성한 데이터

## 빅데이터와 기존 일반 데이터의 차이점

- 빅데이터는 다양한 방법과 원천, 환경으로부터 수집된 데이터를 의미

- 빅데이터는 병렬 처리를 위한 컴퓨터 시스템이 필요할 정도로 큰 데이터임

- 비지니스 or 연구에서 유용한 가치 창출

- 빅.데가 창출하는 가치를 보장하기 위해서는

    - 타당성 validity

    - 신뢰성 veracity

    - 를 확보해야 합니다..

>참고
cpu는 모~든 다른 프로세싱에 관여를 하는데,
gpu는 그 연산밖에 못해요
연산 자체는 단순하겠지만, high quality

>참고: 데이터 단위
비트 Bit: 가장 작은 데이터 단위
8-bit: 1 바이트 Byte, 영어나 숫자는 1바이트, 한글은 2바이트
1킬로바이트(KB) = 1,024 바이트
1메가바이트(MB) = 1,024 킬로바이트(KB)

## 특징

1. 3V: 크키 + 속도 + 다양성

- 크기(Volume)

    - 물리적 장치에 저장되는 데이터의 양

- 속도(Velocity)

    - 데이터의 실시간 처리를 보장할 수 있어야 한다는 것

- 다양성(Variety)

    - 다양한 형태의 데이터를 포함하는 것

```
이제 V가 하나씩 추가됨
```

2. 4V: 3V + 신뢰성

- 신뢰성(Varacity)

    - 데이터가 얼마나 가치 있고 유용한지를 나타내는 것

    - 빅데이터를 분석하는 데 있어, 기업이나 기관에서 수집한 데이터의 정확성을 살펴보는 것

    - 데이터에 대한 신뢰성 + AI에 대한 신뢰성

        - AI를 얼마나 믿을 수 있어? 성능 좋은 건 알겠는데..

        - ex)

            - AI: 당신은 폐암에 걸릴 확률이 90퍼 입니다.

            - 당신: 왜요???!!

            - AI: 그건 설명하기 어렵네요.

3. 5V: 4V + 가치

    - 빅데이터는 비즈니스에 활용되어 가치를 이끌어낼 수 있어야 그 의미가 있음

    - 빅데이터를 설계하고 수집하기 전에, 해당 데이터를 활용하여 무엇을 할 수 있을지에 대한 고민 필요

4. 6V: 5V + 타당성

    - 타당성(Validity)

        - 데이터의 정확성을 의미
    
5. 7V: 6V + 휘발성

    - 휘발성(Volatility)

        - 데이터를 얼마나 오래 저장하고 사용할 수 있는지에 관한 것

- 말고도..

    - 가변성 Variability

        - 데이터의 맥락에 따라 의미가 달라지는 것

        - 어떻게 분석하는지에 따라 다르게 보일 수 있다..

        - ex) 나는 이 데이터를 소나기로 분석했는데 쟤는 쓰나미래

    - 시각화 Visualization

        - 데이터를 시각적으로 표현하는 것

        - 신뢰성 때문에 이게 핫하다네요

## 유형

- 정형 데이터

    - 데이터베이스
    
    - 일정한 형식이나 규칙에 맞게 저장된 데이터

    - 가장 쉽게 접할 수 있는 유형 -> 스프레드시트

    - 정형 데이터의 예: 스프레드시트, 관계형 데이터베이스, CSV 등

- 반정형 데이터

    - 오디오, 비디오

    - 이미지

    - 문서

    - 정해진 규칙이 없어서 값의 의미를 쉽게 파악하기 힘든 경우가 많다..

    - 스키마 & 메타데이터 의 특성을 갖는 데이터

    - 얘는 왜 주목을 받을까

        - 인터넷에 존재하는 수많은 데이터를 인공지능에 사용할 수 있기 때문
    
    - XML, JSON, NoSQL, 로그 등

- 비정형 데이터

    - 메일

    - 웹 페이지

    - 인공지능 기술의 발전과 함께, 비정형 데이터로부터 insight를 얻는 사례가 많아지면서 -> 비정형 데이터에 대한 중요성이 부각되고 있음

- 데이터 분포

    - 비정형 > 반정형 > 정형

# 초거대 데이터와 인공지능

## 인공지능과의 관계

- 인공지능과의 관계

    - 인공지능은 빅데이터에 대한 분석 방법을 제공

    - 인공지능 기술을 이용하면 어떤 사용자라도 쉽고 빠르게 인사이트를 찾아낼 수 있음

1. 인공지능의 주요 원료인 데이터

    - 인공지능 기술은 빅데이터와 결합되어 대량의 데이터 학습을 진행

    - 이를 바탕으로 의사결정을 위한 인사이트 제공 및 미래 상황 예측 가능

2. 인공지능과 융합하는 빅데이터

    - 빅데이터와 데이터 분석 기능을 통해 비지니스 가치를 창출할 때 가장 주목해야 할 기술은 `인공지능과 빅데이터의 융합`

    - 빅데이터와 인공지능의 융합으로 인한 변화

        - 사물인터넷으로의 연결

        - 초연결 산업 생태계로의 전환

        - 사이버 시스템과의 융합

        - 스마트 머신 등장
    
    - 빅데이터 플랫폼을 소유하는 국가나 기업이 4차 산업혁명 시대를 지배할 것..

## 인공지능의 활용

1. 금융 사업

    - 로보어드바이저 Robo Advisor

    - IT + 금융

    - 컴퓨터가 사람을 대신해 자산을 관리

>핀테크 FinTech
금융에 IT 기술을 접목 -> 복잡하고 어려웠던 금융 업무를 효율적이면서 편리하게 서비스
페이코, 토스, 카카오페이

2. 헬스케어 산업

- IBM 왓슨(Watson): 헬스케어 산업에서 빅데이터 + 인공지능 결합 사례

    - 병원 진료 기록, 학계 논문, 생체 데이터 등을 통합 분석 -> 환자의 치료 도움

    - 하지만 현재 서비스 중단 및 임상시험 서비스로 전환(이득을 많이 안겨주지 못해서)

3. 유통 산업

- 이미지를 이용한 비주얼 검색

- 온오프라인 매장에 도입된 채팅봇의 고객 응대

- 아마존고

4. 전자 산업

- 최신 AI 가전제품

- 사용자의 생활 습관이나 공간의 특성 등을 스스로 학습 -> 최적의 작동 방식 찾아냄

- 삼성전자 무풍 에어컨, LG전자 트롬 세탁기

# 초거대 데이터의 개방

## 공공데이터 개방의 필요성

- 빅데이터의 효과를 단기간 내에 얻을 수 있는 방법.. -> 정부가 소유하고 있는 데이터를 활용하는 것

- 정부가 빅데이터를 개방한다면, 민간에서는 큰 수고 없이 서비스를 개발할 수 있을 것

## 공공데이터 개방 사례

- 미국의 데이터 포털사이트 data.gov

    - 농업, 환경, 에너지 등의 분야를 포함

    - 재정 투명성을 위한 예산, 지출, 계약, 공무원 급여까지

>참고
데이터가 방대하면..
데이터의 질이 떨어지는 경향이 있긴 하더라구요

- 영국의 데이터 포털사이트 daga.gov.uk

    - Find Open Data 라는 이름으로 개편 됐대요

# 인공지능 동향

## 인공지능의 투명성과 신뢰성

- 인공지능은 편향된 데이터나 알고리즘의 복잡성으로, 예기치 않은 오류를 발생시킬 가능성이 많음..

- 문제는, 현재의 인공지능이 자신의 결정에 대한 이유를 설명하지 못하는 `블랙박스` 시스템이라는 것

- 인공지능의 학습 과정 및 결과에 대한 투명성이 검증되지 않는 한, 신뢰성은 당연히 제기될 수밖에 없는 문제

    - 누구한테 책임을 묻느냐

- 인공지능 신뢰성 검증을 위한 다양한 정책 및 가이드라인이 발표되고 있지만, 주로 사후대책과 관련된 것이어서.. -> 근본적인 예방이 될 수는 없음

- 인공지능의 시작인 데이터에 대한 검증 -> 인공지능에 대한 신뢰성을 보증하는 시작

## 연합학습 FL, Federated Learning

- 인공지능의 성능은 데이터의 양과 비례

- 데이터는 일반적으로 엣지에서 수집되고 -> 중앙 서버로 전송 -> 이후 중앙 서버에서 데이터를 분석 -> 다시 그 결과를 엣지로 보내는 과정 반복

- 문제.. 이 과정에서 네트워크 과부화 발생

    - 그래서! 고안된 것 -> `연합학습`

- 연합학습

    - 다수의 개별 기기 & 하나의 중앙 서버

        - 얘네들이 협력하여 데이터를 처리, 학습하는 기술

- 연학학습을 이용할 경우, 민감한 데이터를 보호할 수 있다는 장점

    - 특히 의료기관에서 유용하게 사용

- 보유한 데이터를 모두 중앙 서버로 전송하게 되면 -> 네트워크 트래픽, 스토리지 비용 증가 하는데

- 연합학습 사용하면? -> 데이터로 학습된 모델의 업데이트 정보만 ㄴ주고받을 수 있어 -> 네트워크 트래픽에 대한 부담 X

## 모델의 경량화

- 인공지능 모델을 축소시키고 가볍게

- 모델 사이즈를 줄이기 or 가볍게 만들기 -> 추론 시간 Latency 을 줄이는 데 특화

- 어떻게 구현하니?

    - 툴과 함께 -> 경량 알고리즘, 가지치기

    - 툴 없이 -> 양자화, 지식 증류, 순위 근사 등

1. 경량 알고리즘

    - ResNet, DenseNet, MobileNet, ShuffleNet

    - ResNet
        
        - 마이크로소프트에서 개발한 이미지 분류 알고리즘
    
        - 입력값을 출력값에 더하는 지름길을 만드는 것이 핵심

        - 특정 위치의 출력 = 특정 위치의 입력 + 레지듀얼 함수

        - 일반적인 합성곱 신경망에서 사용하는 곱의 연산보다, 단순하고 속도가 빠름

    - DenseNet

        - 모든 이전 계층의 출력 정보를 -> 이후 계층의 입력으로 받아오도록 설계된 알고리즘

        - ResNet과 달리, 모든 계층이 연결되어 있다..

        - 특성맵 Feature Map 의 정보를 합칠 때, 합(+)이 아닌, 채널에서 병합 concat 하는 방식을 사용 -> 연산량 감소 효과

    - MobileNet

        - 컴퓨터 성능이 제한되거나, 배터리 성능이 중요한 기기에 사용될 목적

        - 파라미터를 획기적으로 줄인 알고리즘

        - 기존의 합성곱 필터 -> 채널 단위로 먼저 연산 -> 그 결과를 다시 픽셀 단위로 연산 -> 연산량 감소

    - ShuffleNet

        - MobileNet과 같이, 파라미터 수를 줄인 경량의 알고리즘

        - 픽셀 단위로 합성곱을 할 때, 특정 영역의 채널에 대해서만 연산을 하도록 설계 -> 연산량 감소

2. 가지치기 Pruning

    - 신경망의 파라미터 중, 중요도가 떨어지는 파라미터를 찾아 제거하는 방법

    - 영향도가 적은 파라미터라 하더라도, 제거했을 시.. -> 정확도 손실이 발생할 수 있음

        - 가지치기를 한 상태에서는 추가적인 학습이 필요

            - 시냅스도 가지치기 하고요

            - 뉴런도 가지치기 해요

3. 양자화 Quantization

    - 파라미터의 정밀도를 적절히 줄여 연산 효율성을 높이는 방법

    - 심층신경망에서 학습 및 추론을 진행하면 -> 32비트 or 64비트 부동소수점 수준의 정밀도가 필요하지 않은 경우가 많음

    - 16비트, 8비트의 정밀도를 사용해도 -> 비교적 적은 정확도 손실 -> 고속 연산이 가능

    - 최근 4비트, 2비트, 1비트를 사용하는 알고리즘도 연구중
    
        - 근데 이러면 정확도 손실이 크겠죠?

        - 적은 수의 비트를 고려해서 학습하도록 하는 알고리즘 필요

4. 지식 증류 Knowledge Distillation

    - 미리 잘 학습된 교사 네트워크로부터 학생 네트워크로..

    - 교사-학생 네트워크

    - 학습하고자 하는 데이터가 있을 때,

        - 교사 네트워크보다 작은 규모의 네트워크를 활용
    
5. 순위 근사 Low Rank Approximation

    - 합성곱 신경망에서의 수많은 합성곱 연산은 주로 행렬 곱셈을 사용

    - 순위 근사 에서는 행렬곱 연산 시 랭크 Rank 를 줄여 연산함으로써 -> 근사해를 구하더라도 더 빠른 속도로 연산 가능

>참고
벡터 공간
