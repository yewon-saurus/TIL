# Responsible AI

![Responsible AI](/AI/img/ResponsibleAI.png)

# 인공지능이 내린 결과에 대한 신뢰성

- 미국 전기전자학회인 IEEE는

- 인공지능과 자율시스템의 윤리적, 사회적 이슈를 논의하기 위한 보고서에

- 인공지능이 가져올 변화를 다음과 같이 기술

```
미래의 인공지능 시스템은 세상에 농업 혁명이나 산업 혁명과 맞먹는 정도의

영향을 끼칠 역량을 가지고 있을지 모릅니다.
```

- 인공지능 기술은 4차 산업혁명을 이끄는 핵심

    - 인류의 삶을 바꿀 수 있을 정도의 파괴력을 가진 인공지능이 내놓는 결과는 과연 `신뢰할 만할까?`

- 인공지능의 불완전한 판단

    - 인공지능의 불완전한 판단으로 -> 세계 곳곳에서 사건, 사고 발생

    - 축구공 vs 심판의 민머리
    
    - 자율주행차 사례

        - 최근 자율주행차 시범 운행이 활발히 진행되자..

        - 추돌사고와 사망사고 기사가 늘기 시작
    
    - 의료 인공지능

        - IBM 왓슨

        - 왓슨의 폐암 진단율 불과 17.8%

- 인공지능의 신뢰성

    - 인공지능의 신뢰성 문제는, 학습 방법에서부터 시작

        - 데이터를 이용한 학습: 주어진 대용량의 데이터를 학습하여 모델을 만듦

        - 예측 및 분류: 모델이 만들어진 직후, 또 다른 신규 데이터를 모델에 투입하여 인사이트(예측 및 분류)를 도출

    - 이렇게 만들어진 모델의 정확도.. 100% 였다면 결과에 대한 의구심을 가지지 않았겠죠?

        - 하지만, 인공지능이 인간의 생명과 직결되는 판단들을 내놓으면서

        - 인공지능을 전적으로 믿을 수 있는지에 대한 의구심 증폭

```
계속해서 학습이 돌아가요
계쏙 데이터를 크롤링을 해서 모아서
모델을 계속~ w, b 조절해서 학습을~

이게 좀 더 나아가면 continual learning
평 생 학 습
점점 자기 스스로 강해져요
-> 알파고(우리가 아는 알파고 아니고, 거기서 더 똑똑해짐)

자기가 두는 거 하나 자체가 데이터가 되니까
데이터를 따로 받아올 필요가 없어요?
```

# 인공지능에서의 블랙박스

- 블랙박스의 개념

    - 인공지능이 사물을 인식하는 방식은 인간과 크게 다르지 않은데

    - 그 이유는.. 인공지능도 결국 인간의 신경망을 모방한 것이기 때문

    - 인공지능이 내놓은 결과는, 어떻게 또는 무엇을 근거로 그러한 결과가 나왔는지 정확하게 알 수 없는 게 아닐까?

        - 인간이 어떤 사물을 '고양이'라고 인식할 때, 어떤 과정을 거쳤는지 설명할 수 없는 것

    - 인공지능 시스템이 내놓은 판단이나 결정의 과정 또는 방법에 대해, 적절한 설명을 할 수 없는 상태

    - 인공지능은 매우 복잡해서, 전문가조차 이해하기 어렵다는, 이른바 `블랙 박스` 문제가 발생

- 블랙박스가 발생하는 이유

    - 주어진 데이터의 불완전성

        - 인공지능의 학습 데이터는 인간으로부터 오기 때문에
        
        - 인공지능은 객관성이 떨어짐

        - 데이터 원료 자체에 편향이 개입되므로.. -> 인공지능이 편향을 가지는 것은 불가피

    - 불확실한 학습

        - 인공지능의 학습 과정을 인간이 이해할 수 없음

        - 인공지능의 신경망은 서로 복잡하게 연결된 수백 개의 계층에서 수백만 개의 매개변수들이 상호작용하는 구조 -> 사람이 인지하는 것이 사실상 `불가능`

        - 인공지능의 알고리즘은 불투명해진 상태에서 -> 인간 인지 영역을 넘어선 단계

        - 그냥 넣어 봤는데.. 어? 이거 잘되네? 왜..? -> 알 수 없어..
    
    - 불확실한 데이터 학습 과정

        - 유럽 연합은 개인정보보호 규정인 GDPR에서 유럽 연합 시민은 프로파일링 등 자동화된 처리의 적용을 받지 않을 권리를 갖는다고 규정

        - 인공지능에 의한 잘못된 판단을 위한 보호조치

        - 인공지능의 판단 과정을 이해할 수 없다면? 알고리즘에 오류가 있는지, 어떤 의도로 그러한 판단을 내렸는지 알 수 없음: `Trade-off`

        - 현실적으로 정확성은 떨어지더라도, 그 과정을 설명할 수 있는 방식이 필요

# 설명가능 인공지능의 등장

- 등장 배경

    - 인공지능으로 구현된 프로그램에

    - '나무늘보' 그림을 '레이싱카'라고 학습시킨 결과, 실제로 '나무늘보'를 '레이싱카'로 잘못 판단한 사례

    - '정지' 표지판을 보면 멈추는 자율주행차가, 노이즈 데이터가 추가된 '정지' 표지판을 보고는

    - '최고속도 45'로 잘못 판단한 사례

    - 이러한 오류를 수정하려면.. 인공지능이 결과를 판단하는 근거 및 이유를 알아야 함

- 설명가능 인공지능의 개념

    - 사용자가 인공지능 시스템의 동작과 최종 결과를 이해하고 올바르게 해석하도록

    - 결과물이 생성되는 과정을 설명해주는 기술

        - 결론 과정을 이해할 수 있다?

        - 성공한 이유를 이해할 수 있다?

        - 싪패한 원인을 파악할 수 있다?

        - 신뢰할 수 있다?

        - 오류가 발생한 이유를 알 수 있다?

    - 현재 인공지능은 학습용 데이터를 학습 모델에 투입하여 분석한 후

    - 그 결과를 이용자에게 전달하는데

    - 이 때, 학습 영역에서는 확률값을 계산한 후 이용자에게 전달

        - 객체(고양이) 분석 확률이 93%이기 때문에, '고양이'임을 사용자에게 제시하고 있지만..

        - 이 과정에서 사람은 왜 인공지능이 이런 결과를 도출했는지에 대한 근거를 알 수 없음

- 응용

    - AI 심머신

        - 설.가.인의 대표적인 사례

        - 이 솔루션은 정확한 예측에 이르기까지의 과정을 가시적으로 보여줌
        
        - 시계열 분석을 통해 시간 경과에 따른 변화와 변화 원인을 간격을 두고 설명

    - 자율주행차

- 인공지능 학습 과정의 시각화 방법

    - 미국 DARPA는 설명가능 인공지능(XAI)에 필요한 해석 가능한 모델 중 하나가

    - 그래프 기반 모델이라는 연구 결과를 발표

    - 그래프는 점과 선을 이용한 데이터 시각화 표현으로

    - 학습 과정을 시각적으로 표현함으로써 설명가능 인공지능을 구현 가능

    - 신경망 노드에 설명 붙이기

        - 인공신경망에서 설명 가능한 노드를 찾아

        - 설명을 붙이는 방법

        - 예) 인공신경망은 고양이의 수염, 털, 발톱과 같은 이미지의 특정 부분에 특정 노드를 지정하고 -> 퍼즐 맞추기 게임처럼 모~든 노드를 조합하여 대상 인식

    - 모델 유추하기

        - 인공지능 블랙박스에서 설명가능 모델을 유추하는 방법

        - 모델 유추의 학습 진행

            - 설명 딱지가 붙어 있는 네트워크 학습

            - 딥러닝 시스템을 훈련해 시스템이 어떻게 최종 결론에 도달했는지 설명

- DARPA에서 제공하는, 인공지능 역량평가 요소

    - 사용자 만족도

        - 설명이 얼마나 명확한가?

        - 설명이 얼마나 유용한가?

    - 설명모델 수준

        - 개별 의사결정 이해도

        - 전체 모델에 대한 이해도

        - 장단점 평가

        - '미래 해동' 예측

        - '개입 방법' 예측

    - 업무수행 향상도

        - 설명이 사용자 의사결정, 업무수행 능력을 향상시켰는가?

        - 사용자의 이해도 평가를 위한 실험적 업무

    - 신뢰성 평가

        - 미래에도 사용할 만큼 신뢰하는가?

    - 오류 수정 수준(가점)

        - 인식 오류 수준

        - 인식 오류 수정을 위한 지속적인 훈련

# Ethics for AI

p26

# 인공지능 특이점

- 특이점의 개념

    - 인공지능이 인간의 지능을 초월하는 시점

    - 사람이 기술의 발전을 따라잡을 수 없는 시기

    - 특이점에 도달하는 시대에는.. 컴퓨터가 자신보다 발달한 인공지능을 직접 설계 및 제작할 것이며, 그 인공지능은 자신보다 더 좋은 지능을 가진 또 다른 인공지능을 설계하고 생산할 수 있게 될 것

        - 우리가 특이점을 대비하고 준비해야 하는 이유는.. 인공지능에 대한 `통제력`과 관련이 있음

- 특이점의 쟁점

    - 인공지능의 특이점에 대한 논쟁..

    - 인공지능 개발이 본격호 되면서 더욱 심화

    - 반 인공지능파 vs 친 인공지능파

    - 인공지능의 통제: 킬 스위치

        - 인공지능이 이상반응을 보일 때 외부에서 강제로 종료시키는 것

        - 이런건.. -> 전원을 꺼놔도 자가발전 하는 지능을 갖췄다거나, IoT에 스스로 접속하여 스스로 대비를 한다든지 하면? 의미가 없어지겠죠

# 인공지능 윤리의 필요성

- 주목받는 인공지능 윤리

    - '이루다'

    - 프로퍼블리카

    - 아마존의 채용 인공지능

- 필요성

    - MS 최고 경영자인 사티아 나델라는, 인공지능 윤리에 대한 화두를 던짐

    - "인공지능 활용에 앞서, 윤리가 우선시돼야 한다."

    - 구글 이미지 인식 사례

        - 흑인 엿어을 고릴라로 인식

    - 위챗 사례
        
        - 니그로 라는 단어를 사용

    - AI 알고리즘은.. 어떤 데이터를 입력하는지에 따라 결과가 달라짐

        - 인공지능에게 어떠한 데이터를 주입할 것인지는 인간의 몫

# 인공지능의 윤리적 딜레마

- 트롤리 딜레마

    - 윤리학 분야의 사고실험 중 하나

# 인공지능 윤리안

- 아실로마 인공지능 원칙

    - 인공지능 개발의 목적, 윤리, 가치 등에 대해 개발자가 지켜야 할 23가지 준칙

        - 이 원칙은 연구 관련 쟁점, 윤리와 가치, 장기적 이슈 등 총 3가지 부분으로 구성

    1. 연구 관련 쟁점

        - 연구 목표, 연구비 지원, 과학정책 연계, 연구 문화, 경쟁 회피 등

    2. 윤리와 가치

        - 안전, 실패의 투명성, 사법적 투명성, 책임성, 가치 일치, 인간의 가치, 개인정보보호, 자유와 프라이버시, 이익의 공유, 번영의 공유, 인간 통제, 사회전복 방지, 인공지능 무기 경쟁 지양 등

    3. 장기적 이슈

        - 역량 경고, 중요성, 위험성, 자기개선 순환, 공동의 선 등

- 로봇의 3원칙

    - The Three Laws of Robotis

    - 로봇이 반드시 따라야 할 3가지 원칙

    1. 제1원칙: 로봇은 인간에게 해를 입혀서는 안 되고, 위험에 처한 인간을 방치해서도 안 된다.

    2. 제2원칙: 제1원칙을 어기지 않는 한, 로봇은 인간의 명령에 복종해야 한다.

    3. 제3원칙: 제1원칙과 제2원칙을 어기지 않는 한, 로봇은 로봇 자신을 지켜야 한다.

    - 이후, 아이작 아시모프는 `로봇과 제국`에서 로봇 0원칙을 추가 제안함

        - 로봇 0원칙: 로봇은 인류에게 해를 가할 만한 명령을 받거나, 행동을 하지 않음으로써 '인류'에게 해가 가해지는 것을 방치해서도 안 된다.

            - 제1원칙의 확장
