# 지능형 에이전트

- 사람의 지능 행위와 지능 에이전트

    - 사람은 주위를 센싱 -> 인식 -> 적절한 행위

    - 위와 같은 과정을 통해 -> 환경과 상호작용

    - 인공지능 제품도.. 비슷한 과정을 수행할 수 있어야 -> 쓸모가 있다

# 지능형 에이전트 시나리오

- 일상생활에서

    - 특정한 일을 대행해주는 사람을 -> `에이전트`라고 부름

    - 예, 여행 에이전트

- 컴퓨터에서 사람 일을 대신하는 소프트웨어를 에이전트라 부름

    - 사람의 지능적인 일을 대신해주는 지능 에이전트에 대해 다룸

- 지능 에이전트

    - 로봇은 에이전트, 환경, 지식 베이스가 명확히 구분

        - 로봇 몸과 소프트웨어가 Agent, 카메라 등이 센서, 로봇 팔이 Actuator

    - 음성 인식 앱은 -> 구분이 조금 모호

        - 스마트폰과 앱이 Agent, 카메라는 Sensor, 화면과 스피커는 Actuator

- 지식 베이스

# 강화 학습

- 자신이 한 행동에 대해 reward를 받으며 학습하는 것

- 컴퓨터가 주어진 상태에 대해 -> 최적의 행동을 선택하도록 학습하는 방법

- 에이전트, 상태, 행동, 보상, 환경, 관찰

- 업무 수행

    - task는 상태 변화와 행동을 번갈아 수행하면서 목적을 달성

    - 지도 학습이나 비지도 학습으로는 task 달성 불가능

        - 그래서 강화 학습을 하는구나~
        
        - 아기가 '걷기'를 학습할 때

            - 오 이렇게 잡고 서니까 편하네

            - 이런식으로 학습

            - 인간은 지속적으로 강화학습을 하고 있는 거구나

            - 로봇은 정해진 시나리오가 있어서, 끝맺음을 한다고요?

- 환경과 상호작용하며 살아가는 인간과 동물

    - 환경의 상태를 보고 -> 자신에게 유리한 행동을 결정 -> 실행

    - 행동에 따른 결과가 좋으면? -> 기억했다가 반복

    - 나쁘면? -> 회피

        - 자전거 타기, 스키너의 행동심리학 실험, 바둑, 비디오 게임, ...

    - 행동 -> 상태 변화 -> 보상의 학습 사이클

- 컴퓨터로 이런 방식의 학습을 할 수 있을까?

    - 지금까지 공부한 지도 학습

        - 다층 퍼셉트론.. 컨볼루션 신경망.. 순환 신경망..

        - 이런건 부적절함

    - `강화 학습`이 대안

- 강화 학습의 응용

    - 스타크래프트와 같은 온라인 게임

    - 장기나 바둑(알파고), 윷놀이

    - 지능형 로봇 제어

    - 자동차의 자율 주행

        - 트롤리 문제 같은 경우는 강화 학습을 못 해요

            - value가 같아서

            - 이건.. 인공지능 윤리학의 문제

    - 스마트 팩토리 제어(정유 생산 라인의 제어)

# 학습 원리와 성질

- 강화 학습의 핵심 연산

    - 시간에 따른 state, action, reward의 순차적 처리

- 보상 책정 망안

    - 매 순간 보상액 책정이 어려우면 -> 중간에는 보상액을 0으로 설정하고 -> 마지막 순간에 보상액 결정

    - 예: 걷기 task 에서, 중간에는 보상액 0, 마지막 순간에는 아빠에게 안기면 1, 넘어지면 -1 부여

    - 예: 바둑에서 중간에는 보상액 0, 마지막에 이기면 1, 지면 -1 부여

- 에이전트와 환경

    - 에이전트는 정책을 가지고 행동을 결정

        - 정책은 학습으로 알아내야 함

    - 환경은 MDP(Markov decision process)를 가지고 상태 전환과 보상액 결정

        - MDP는 주어지는 것..

        - `지도 학습의 훈련 집합` 에 비유하면 되겠음

- 강화 학습의 목표는 `누적 보상액을 최대화`하는 것

    - 이를 달성하려면, 매 순간 좋은 행동을 취할 수 있어야 함

        - 정책에 따라 행동을 결정하므로, 좋은 정책이 필요한셈

    - 강화 학습은 주어딘 MDP를 가지고 최적 정책을 찾아야 함

        - 지도 학습이 훈련 집합을 가지고 최적 매개변수를 찾는 것에 비유할 수 있음

- 상태, 행동, 보상의 표현

    - 대부분 응용에서는 -> 이산 값을 가짐

        - 상탯값 집합

        - 행동값 집합

        - 보상값 집합
    
    - 어떤 순간에 어떤 상태에 있다가 다음 순간에 그런 상태로 바뀔 확률

    - 어떤 순간에 어떤 상태에서 어떤 행동을 취했을 때 그런 상태로 바뀔 확률

- 정책 policy

    - 에이전트는 확률 규칙으로 행동을 결정하는데

    - 이 규칙을 정책이라고 함

    - 동, 서, 남, 북 이라는 4가지 행동에 대해 동일 확률을 사용한다고 가정했을 때

        - 각 행동에 동일 확률을 쓰는 정책.. 과연 좋은 정책인가?
    
# 탐험과 탐사

- 탐험과 탐사 갈등 현상

    - 탐험은 `전체 공간`을 골고루 찾아보는 전략

    - 탐사는 `특정한 곳` 주위를 집중적으로 찾아보는 전략

        - 무작위 탐색 알고리즘은 극단적인 탐험 방식, 경사 하강법은 탐사 방식

        - 탐험은 시간이 너무 오래 걸리고, 탐사는 지역 최적해에 머무는 문제

- 탐사로 치우치지 않게 주의를 기울여야 함

    - k-손잡이 벤딧 문제

- 탐험형 정책과 탐사형 정책

    - 양 극단

        - 처음부터 끝까지 무작위로 선택하는 => 극단적인 탐험형 정잭

        - 몇 번 시도해 보고 이후에는 그때까지 승률이 가장 높은 손잡이만 당기는 -> 극단적인 탐사형 정책

    - 둘 사이의 균형이 중요

        - 현재까지 높은 확률을 보인 손잡이를 더 자주 당기지만 -> 일정한 비율로 다른 손잡이도 시도하는 정책

- 에피소드가 충분히 긴 경우는 해법이 단순

    - 강화 학습에서는 게임을 시작하여 마칠 때까지 기록을 에피소드라 부름

    - 충분히 긴 에피소드로부터 승률을 계산하여 -> 이후에는 승률이 가장 높은 손잡이만 당김

        - 에피소드가 충분히 길다면 -> `최적 정책`을 알아낼 수 있음

        - 근데.. 주인이 수시로 확률을 바꾼다거나, 돈과 시간이 충분하지 않은 경우가 태반

# 계산 모형

- 몬테카를로 방법

    - 현실 세계의 현상 또는 수학적 현상을

    - 난수를 생성하여 시뮬레이션 하는 기법

    - random 함수와 epsilon greedy 함수 -> 몬테카를로 방법

    - 인공지능은 다양한 목적으로 몬테카를로 방법 활용

        - 몬테카를로 트리 탐색

        - 바둑을 할 때

            - 탐험 or 탐험 시 -> 난수를 생성

            - 오 여기 놔볼까?

            - 계산

            - 오 여기 놔야겠다

- 마크로프 결정 프로세스 MDP

    - 상태의 종류, 행동의 종류, 보상의 종류 -> 지정하고,

    - 행동을 취했을 때 발생하는 상태 변환을 지배하는 규칙을 정의

    - 상태와 환경, 보상

        - 보상이 주어지는 시점

            - 즉시 보상: 예) 다중 손잡이 밴딧 -> 당기면 바로 나옴

            - 지연된 보상: 예) FrozenLake, 바둑, 장기, 비디오 게임 등(대부분 문제가 지연 보상) -> 끝까지 봐야 알아요

# 정책과 가치 함수

- 강화 학습의 핵심 -> `좋은 정책을 갖는 것`

- 좋은 정채기 있으면? -> 누적 보상액을 최대로 만들 최적 행동을 매 순간 선택할 수 있음

- 좋은 정책을 찾는 일.. -> 지도 학습이 목적함수의 최적해 찾는 일에 비유 OK?

- 좋은 정책

    - 누적 보상을 최대화하려고 -> 일부러 함정에 빠지는 행동까지 추론 가능해야 함

    - 예, 바둑에서 작은 말을 희생하여 대마를 잡는 상황

# 강화 학습과 지도 학습의 비교

![강화학습vs지도학습](/AI/img/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5vs%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5.png)

# 가치함수와 Q함수

- 가치 함수(상태 가치함수)

    - 특정 상태에 대한 '가치'를 계산해주어 -> 에이전트가 행동을 선택할 수 있게 도와주는 것

- Q함수(행동 가치함수)

    - 특정 상태 s에서 특정 행동 a를 취했을 때

    - 받을 반환값에 대한 기댓값으로

    - 특정 행동 a를 했을 때 얼마나 좋을 것인지에 대한

- 아무튼..

    - 학습 알고리즘이 할 일?

    - 바로.. `최적의 정책을 찾는 것`

- 정책 공간은 방대하여 -> 모~든 정책을 일일이 평가하고 -> 그 중 제일 좋은 것을 선택하는 방법은 현실성이 없음

    - 따라서, 가치함수를 이용하는 전략 사용

# 응용 사례

- TD=gammon: 자가 플레이로 학습한 최초 사례

- DQN: 같은 신경망으로 다른 수십 종의 게임에서 높은 성능을 얻음

