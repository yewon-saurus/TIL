# Preview

- 생물 신경망에서 인공 신경망 태동

    - 인공신경망의 태동 또는 인공지능의 태동

- 컴퓨터의 등장으로 실제 구현 시도

    - 퍼셉트론.. 가장 성공한 인공 신경망 모델

    - 퍼셉트론은 발전을 거듭해 -> 현재 딥러닝으로 이어짐

    - 이 장에서는 `퍼셉트론`과 `다층 퍼셉트론`을 다룬다..

    - 얇은 신경망

# 생물 신경망

- 사람 뇌와 컴퓨터

    - 뉴런 neuron 은 뇌의 정보처리 단위로서,
    
    - 연산을 수행하는 세포체 soma,
    
    - 처리한 정보를 다른 뉴런에 전달하는 축삭 axon,
    
    - 다른 뉴런으로부터 정보를 받는 수상돌기 dendrite 로 구성

    - 사람 뇌는 10^11 개 가량의 뉴런,

    - 뉴런마다 1,000개 가량의 연결 -> 고도의 병렬 처리기

    - 반면에, 폰 노이만 컴퓨터 -> 아주 빠른 순차 명령어 처리기

>참고
인공 신경망의 발상과 전개
디지털 컴퓨터와 역사를 같이 하는 인공 신경망

# 인공신경망의 역사

## 퍼셉트론의 등장

- 1957년, 프랭크 로젠블라트

- 인간의 두뇌 움직임을 수학적으로 구성하여.. -> 당시 굉장한 이슈

- 인공신경망에 대한 기대 폭증

- 신경망의 기원이 되는 알고리즘

    - 입력 input

    - 가중합 weighted sum

    - 활성화 함수 activation function

    - 출력 output

- 초기 퍼셉트론을 이용한 문제해결 -> AND, OR, ... 간단한 연산!

- 당시에 이정도만 할 줄 알면, 이리저리 조합해서 어떤 문제든 풀어낼 수 있다고 생각함

- AND, OR는 선형 분리가 가능한데..

    - 직선 하나로 데이터들이 두 개의 그룹으로 나눌 수 있음

    - XOR는 선형 분리 불가

    - 퍼셉트론 여러 개 쌓아 올리면 ㄱㅊ긴 한데요

        - 각각의 가중치 weight

        - 편향 bias

        - 을 학습시킬 방법이 없다 -> 한계

# 퍼셉트론의 구조와 연산

## 퍼셉트론의 구조

- 입력층 + 출력층(출력층은 한 개의 노드)

- 입력층은 d + 1 개의 노드

    - d는 특징 벡터의 차원

- i번째 입력 노드와 출력 노드 -> 가중치 w(i)를 가진 에지로 연결됨

- 계단 함수를 활성 함수로 사용

## 퍼셉트론의 연산

- i번째 에지는 X(i)와 W(i)를 곱해 출력 노드로 전달

- 0번째 입력 노드 X(0)은 1인 바이어스 노드

- 출력 노드는 d + 1 개의 곱셈 결과를 모두 더한 s를 계산하고 활성함수 activation function 적용

- 활성 함수

    - 뉴런을 활성화하는 과정을 모방

    - 퍼셉트론은 활성 함수로 -> `계단 함수` 사용

        - s가 0보다 크면 1, 그렇지 않으면 -1 출력

    - 따라서 -> 퍼셉트론은 특징 벡터를 1 or -1 로 변환하는 장치.. -> 즉, 이진 분류기 인 것

# 퍼셉트론으로 인식하기

- 퍼셉트론은 이진 분류기 binary classifier

    - 이진 분류 문제의 예

        - 생산 라인에서 나오는 제품을 -> 우량 vs 불량 으로 구분

        - 병원에 온 사람을 -> 정상인 vs 환자 로 구분

        - 내가 등장하는 사진 을 구별해 냄

    - `다중분류는 적용 불가`

- 퍼셉트론의 인식 능력

    - 제품을 -> 크기 와 색상 을 나타내는 두 개의 특징으로 표현한다고 가정

    - 이 경우, d = 2

    - 특징 값은 0 or 1 이라고 가정

    - x = (크기, 색상) = (x<sub>1</sub>, x<sub>2</sub>)

    - 생산라인에서 제품 4개를 수집하여 관찰한 결과 -> 다음 데이터를 확보

        - x<sub>1</sub> = (0, 0), x<sub>2</sub> = (0, 1), x<sub>3</sub> = (1, 0), x<sub>4</sub> = (1, 1) -> 특징 벡터

        - y<sub>1</sub> = 1, y<sub>2</sub> = 1, y<sub>3</sub> = 1, y<sub>4</sub> = 1

            - 레이블 y = 1은 불량, y = -1은 우량

![데이터를 인식하는 퍼셉트론](/AI/img/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0x2%EC%83%98%ED%94%8C%EC%98%88%EC%B8%A1.png)

- 가중치를 이리저리 대입하고 o를 0으로 설정하고 어쩌구

- 아무튼, 퍼셉트론의 결정 경계(특징 공간을 분할하는 경계) -> 직선의 방정식으로부터..

- 퍼셉트론은 특징 공간을 `두 부분 공간`으로 분할하는 `이진분류기` 이구나~

- 퍼셉트론은 `선형으로 국한됨`

```
참고

weigth를 어떻게 설정해요?

이거 랜덤하게 설정해주는 거예요

처음에 initialize 할 때 컴퓨터가 임의로 뿌리고요

우리가 실력을 좀 갖추고 나면 이것도 조정을 해줄 수 있답니다..
```

# 행렬 표기

## 행렬 연산의 필요성

- 대부분 기계 학습 책 -> 행렬 표기 사용 해요

- 파이썬 언어 -> 주로 행렬 연산 사용하여 코딩

- 수학적인 background가 쓰이는구나.. 선형 대수가..

- 아무튼 이래저래.. 퍼셉트론의 동작을 행렬로 쓸 수도 있다..

- 파이썬 `numpy`

# 사람의 학습과 신경망의 학습

## 퍼셉트론의 학습

- 데이터와 함께 데이터를 인식하는 퍼셉트론(가중치)이 주어짐

    - 즉, 데이터로 학습을 마친 퍼셉트론이 주어진다는 말

- 실제 상황에서는 데이터만 주어지므로, 학습 알고리즘으로 가중치를 알아내야 함~~

    - OR 데이터의 경우 -> 2차원 특징 벡터이고, 샘플이 4개 뿐이라 -> 그냥 연필로도 가중치를 쉽게 알아낼 수 있음

    - sklearn의 필기 숫자는.. -> 64차원 특징 벡터, 1,797개 샘플 -> 학 습 알 고 리 즘 없 이 는 안 돼

![사람vs퍼셉트론](/AI/img/%EC%82%AC%EB%9E%8Cvs%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.png)

- 조금씩 나은 방향을 찾아 개선해 나가는 절차.. -> 사람과 유사 하네요

- 근데, 신경망의 학습은? -> 최적화 문제.. -> 최적화 대상은? -> 신경망의 가중치(매개변수)

    - 철저히 수학에 의존한다는 점에서 사람과 다름

![신경망의 학습](/AI/img/%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98%ED%95%99%EC%8A%B5.png)

- 구현에 필요한 사항

    - 1행에서, 초기화는 어떻게? 4행에서, 멈춤 조건은?

    - 3행의 손실 함수 정의
    
        - 보통은 오류의 양을 보고 따짐

    - 5행에서 w의 변화량을.. 어떻게 구하나?

        - 미분을 사용

# 역전파의 등장

## 역전파의 고안

- 역전파 backpropagation

    - 가중치와 편향을 학습하기 위한 방법으로 고안됨

    - 신경망의 오차(|예측값 - 실제값|)를 출력층에서부터 입력층으로 -> 거꾸로 전파 -> 각 층의 가중치와 편향을 업데이트

- 신경망이 깊어질수록 학습력이 좋아져야 하는데 -> 기대하는 결과가 나오지 않는 일이 계속계속 발생

- 역전파 수행 시, -> 출력층에서 멀리 떨어진 층에서는

    - 기울기가 급속히 작아지는

        - 기울기 소멸 문제 vanishing gradient

    - 기울기가 너무 커지는

        - 기울기 발산 exploding gradient
    
    - 문제가 발생.. -> 그래서 학습력이 떨어진 거구나~

- 그럼 우리 이렇게 하자

    - `기울기 자체를 없애는 거임!!`

    - `ReLu`

```
+)

반복을 기준으로 학습을 한다

이것이 바로 epoch

이걸 몇 번 도냐를 설정을 하죠 우리는

계속 반복을 하면서.. -> 학습을 해요


epoch 자체가 굉장히 적게 돌아갔다?

weight가 빨리 설정이 된 거예요

적게 돌았는데 early stopping이 된다?

-> 하나의 overfitting 문제로 이어질 수도..
```

```
+)

RNN

지금까지 있었던 일들을 모~두 가지고 있는 것


중간 중간에 필요 없는 정보는 forget 하자..

-> LSTM

그럼 여기서는, 얼마나 잊게 할 지 정하는 것이 중요하겠죠


여기서 더 개선된 게 GLU?
```

- 회귀모델에서 오차를 계산하는 방법으로는

    - MSE

        - Mean Squared Error

        - 예측값과 실제값의 차이인, 오차들의 제곱에 대한 평균

        - 직관적이고 단순함

        - 근데, 오류를 제곱하기 때문에.. -> 1 미만의 오류는 더 작아지고, 그 이상의 오류는 더 커지는.. -> 값의 왜곡 발생

        - 스케일에 의존적

    - MAE

        - Mean Absolute Error

        - 예측값과 실제값의 차이인, 오차들의 절댓값에 대한 평균

        - 매우 직관적인 지표

        - 얘도 스케일에 의존적임 ㅋㅋ

- 기울기를 계산한다는 게 뭘까?

    - 경사 하강법은 오류가 작아지는 방향으로 가중치(w)값을 보정하기 위해 사용

    - 경사 하강법은, 최초 가중치에서부터 미분을 적용 -> 이 미분값이 계속 감소 감소 감소 하는 방향으로 순차적으로 가중치 값을 업데이트 -> 업데이트가 끝나는 전역 최소점에서의 w를 반환

    - 미분값(기울기)가 0이 된다.. -> 더이상 업데이트 X -> weight가 고정됨

![기울기 계산](/AI/img/05%EA%B8%B0%EC%9A%B8%EA%B8%B0%EA%B3%84%EC%82%B0.png)

- 손실함수에는 `지역 최소점 local minimum`과 `전역 최소점 global minimum`이 있는데

    - 지역 최소점

        - 함수 일부 구간의 최솟값

        - 극솟값? 같은 건가
    
    - 전역 최소점

        - 전체 구간에서의 최솟값

# 딥러닝의 등장

## 딥 deep 의 출현

- 역전파를 고안했던 제프리 힌튼이.. -> 가중치의 초기값을 제대로 설정하면! 깊이가 싶은 신경망도 학습이 가능하다는 연구를 선보임

- 이후 뭐 autoencoder 이런거

    - 입력층과 출력층이 동일한 네트워크에, 데이터를 입력하여 비지도학습을 하는 것

    - 가중치의 좋은 초기값을 얻는 목적으로 이용

- 인공신경망 이라는 말 대신 Deep 이라는 용어를 사용하기 시작

# 딥러닝의 개요

## 인공신경망의 개념

- 딥러닝의 기원은 인공신경망 이구나~

- 인간의 뇌 -> 수많은 뉴런이 존재

- 그 뉴런들이? 시냅스로 서로 연결

    - 우리는 이걸 신경망 이라고 부르기로 했어요

- 인공신경망

    - 사람의 신경망 구조에서 착안해 만들어짐

    - 뉴런들의 연결, 즉 신경망을 인공적으로 흉내낸 것

    - 인간의 신경망을 흉내 낸 머신러닝 기법

        - 가지돌기 ~> 입력층

        - 축삭말단 ~> 출력층

        - 축삭말단에 이르기까지의 신호의 크기 ~> 가중치

## 딥러닝의 개념

- 딥러닝 deep learning

    - 여러 층(특히 은닉층이 여러개)을 가진 인공신경망을 사용하여 -> 머신러닝 학습을 수행하는 것

    - 심층학습 이라고도 함

1. 가중합

    - 입력값과 가중치를 곱한 뒤, 편향 bias 을 더한 값

2. 활성화 함수

    - activation function

    - 입력 신호가 출력 결과에 미치는 영향도를 조절하는 매개변수

    - 왜 이걸 쓰나요??

        - 출력값을 0 ~ 1 사이의 값으로 반환해야 하는 경우에 사용

        - 비선형을 위해 사용

            - 선형함수로는, 은닉층을 여러 개 추가해도 은닉층을 1개 추가한 것과 차이가 없거든요

            - 정확한 결과를 위해선 비선형 함수, 즉 활성화 함수가 필요함

    1. 시그모이드 sigmoid 함수

        - x값의 변화에 따라, 0에서 1까지의 값을 출력하는 S자형 함수

        - 로지스틱 함수라고도 부름

        ![sigmoid](/AI/img/sigmoid.png)

    2. 하이퍼볼릭 탄젠트 hyperbolic tangent 함수

        - 시그모이드 함수와 유사

        - 하지만, -1 ~ 1 의 값을 가지면서 -> 데이터의 평균이 0이라는 점이 다름

        ![hyperbolic tangent](/AI/img/hyperbolicTangent.png)

    3. 렐루 ReLU 함수

        - x가 음의 값을 가지면 -을 출력

        - 양의 값을 가지면 x를 그대로 출력

        - 함수 형태도.. `max(0, x)`로 계산이 간단 -> 학습 속도가 매우 빠름!!

        ![ReLU](/AI/img/ReLU.png)

    4. 리키렐루 leaky ReLU 함수

        - 렐루와 유사

        - 차이점: 가중치 곱의 합이 0보다 작을 때의 값도 고려함!!

        - 이 함수는.. 렐루 함수의 `죽은 렐루` 현상을 보완

    - 소프트맥스 softMax 함수

        - 입력받은 값이 0 ~ 1 사이의 값으로 출력되도록

            - 정규화하여 출력의 총 합이 1이 되는 특성을 갖는 함수
        
        - 딥러닝에서 출력 노드의 활성화 함수로 많이 사용

## 딥러닝 학습

1. 순전파

    - forward propagation

    - 입력층에서 출력층 방향으로 -> 연산이 진행되면서 -> 최종 출력값(예측값)이 도출되는 과정

    - 역전파 반대겠구만

2. 손실함수

    - loss function

    - 예측값과 실제값의 차이를 구하는 함수

        - 두 값의 차이가 클 수록 손실함수의 값은 커짐

        - 두 값의 차이가 작을수록 손실함수의 값도 작아짐

    - 그렇겠지..

3. 옵티마이저

    - optimizer

    - 딥러닝에서 학습 속도를 빠르고 안정적이게 만드는 것

    - 전체 데이터(전체 배지)를 가지고 매개변수(가중치, 편향)의 값을 조정할 수 있고

        - 정해준 양의 데이터(미니 배치)만 가지고 매개변수의 값을 조정할 수 있음

            - batch: 가중치 등 매개변수 값을 조정하기 위해 사용하는 데이터의 양
    - 일반적으로, 전체 데이터를 한 번에 학습하게 되면 -> 계산량이 많아질 뿐만 아니라 속도도 느려짐 -> 미니 배치를 이용한 학습을 함

    - 데이터를 조금씩 쪼개서 가져와 학습

4. 역전파

    - backpropagation

    - 오차예측값과 실제값의 차이를 역방향으로 전파

        - 출력층에서 은닉층을 거쳐 입력층으로..

        - 가중치를 업데이트
    
    - 경사 하강법을 이용

    - 가중치 업데이트 시,

        - 순전파에서 계산한 결과 y = f(x)의 편미분값을 오차에 곱해

        - 출력층 -> 은닉층 -> 입력층 순서로 전달
